{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da2eea63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T04:57:10.422109Z",
     "iopub.status.busy": "2024-08-21T04:57:10.421844Z",
     "iopub.status.idle": "2024-08-21T04:57:23.972449Z",
     "shell.execute_reply": "2024-08-21T04:57:23.971562Z"
    },
    "papermill": {
     "duration": 13.557809,
     "end_time": "2024-08-21T04:57:23.974744",
     "exception": false,
     "start_time": "2024-08-21T04:57:10.416935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycocotools\r\n",
      "  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\r\n",
      "Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.7.5)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.26.4)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.47.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\r\n",
      "Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.8/427.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pycocotools\r\n",
      "Successfully installed pycocotools-2.0.8\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c51d93e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T04:57:23.985130Z",
     "iopub.status.busy": "2024-08-21T04:57:23.984403Z",
     "iopub.status.idle": "2024-08-21T04:57:31.015807Z",
     "shell.execute_reply": "2024-08-21T04:57:31.015007Z"
    },
    "papermill": {
     "duration": 7.039069,
     "end_time": "2024-08-21T04:57:31.018192",
     "exception": false,
     "start_time": "2024-08-21T04:57:23.979123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24387864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T04:57:31.028068Z",
     "iopub.status.busy": "2024-08-21T04:57:31.027381Z",
     "iopub.status.idle": "2024-08-21T04:57:31.041174Z",
     "shell.execute_reply": "2024-08-21T04:57:31.040508Z"
    },
    "papermill": {
     "duration": 0.0206,
     "end_time": "2024-08-21T04:57:31.043036",
     "exception": false,
     "start_time": "2024-08-21T04:57:31.022436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, image, target):\n",
    "        image = torchvision.transforms.functional.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "# Lớp Dataset tùy chỉnh cho YOLO\n",
    "class YOLODataset(Dataset):\n",
    "    def __init__(self, image_path, anotation_path, transforms=None, keep_rate=(10, 10)):\n",
    "        self.transforms = transforms\n",
    "        self.img_path = image_path\n",
    "        self.annotation_path = anotation_path\n",
    "        self.keep_rate = keep_rate\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_path[idx]\n",
    "        annotation_path = self.annotation_path[idx]\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = torchvision.transforms.Resize((1500, 1000))(img)\n",
    "        w, h = img.size\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        with open(annotation_path) as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                class_id = int(parts[0])\n",
    "                if random.random() > self.keep_rate[class_id]:\n",
    "                    continue\n",
    "                    \n",
    "                x_center, y_center, width, height = map(float, parts[1:])\n",
    "                \n",
    "                # Convert from YOLO format to (xmin, ymin, xmax, ymax)\n",
    "                xmin = (x_center - width / 2) * w\n",
    "                ymin = (y_center - height / 2) * h\n",
    "                xmax = (x_center + width / 2) * w\n",
    "                ymax = (y_center + height / 2) * h\n",
    "\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                labels.append(class_id)\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            # If there are no boxes, return a dummy box to avoid errors\n",
    "            boxes = torch.tensor([[0, 0, 1, 1]], dtype=torch.float32)\n",
    "            labels = torch.tensor([0], dtype=torch.int64)  # Use a background class (0)\n",
    "        else:\n",
    "            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71672567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T04:57:31.051854Z",
     "iopub.status.busy": "2024-08-21T04:57:31.051588Z",
     "iopub.status.idle": "2024-08-21T04:57:31.455868Z",
     "shell.execute_reply": "2024-08-21T04:57:31.454852Z"
    },
    "papermill": {
     "duration": 0.41174,
     "end_time": "2024-08-21T04:57:31.458763",
     "exception": false,
     "start_time": "2024-08-21T04:57:31.047023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_path(img_path):\n",
    "    images = []\n",
    "    for root, dirs, files in os.walk(img_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".JPG\"):\n",
    "                full_path = os.path.join(root, file)\n",
    "                images.append(full_path)\n",
    "    labels = [path.replace('images', 'labels').replace('.JPG', '.txt') for path in images]\n",
    "    return images, labels\n",
    "\n",
    "train_image_path, train_annotation_path = get_path(\"/kaggle/input/riceseed/images/train\")\n",
    "val_image_path, val_annotation_path = get_path(\"/kaggle/input/riceseed/images/val\")\n",
    "test_image_path, test_annotation_path = get_path(\"/kaggle/input/riceseed/images/test\")\n",
    "num_classes = 2\n",
    "\n",
    "train_dataset = YOLODataset(train_image_path, train_annotation_path, transforms=ToTensor(), keep_rate=(10, 0.83))\n",
    "val_dataset = YOLODataset(val_image_path, val_annotation_path, transforms=ToTensor())\n",
    "test_dataset = YOLODataset(test_image_path, test_annotation_path, transforms=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b61557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T04:57:31.469022Z",
     "iopub.status.busy": "2024-08-21T04:57:31.468698Z",
     "iopub.status.idle": "2024-08-21T04:57:31.483939Z",
     "shell.execute_reply": "2024-08-21T04:57:31.483123Z"
    },
    "papermill": {
     "duration": 0.022585,
     "end_time": "2024-08-21T04:57:31.485817",
     "exception": false,
     "start_time": "2024-08-21T04:57:31.463232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_mAP(predictions, targets):\n",
    "    metric = MeanAveragePrecision()\n",
    "    metric.update(predictions, targets)\n",
    "    return metric.compute()\n",
    "\n",
    "def calculate_metrics(predictions, targets, iou_threshold=0.5):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    for pred, target in zip(predictions, targets):\n",
    "        pred_boxes = pred['boxes']\n",
    "        pred_labels = pred['labels']\n",
    "        target_boxes = target['boxes']\n",
    "        target_labels = target['labels']\n",
    "        \n",
    "        if len(pred_boxes) == 0 or len(target_boxes) == 0:\n",
    "            false_positives += len(pred_boxes)\n",
    "            false_negatives += len(target_boxes)\n",
    "            continue\n",
    "        \n",
    "        ious = box_iou(pred_boxes, target_boxes)\n",
    "        max_ious, max_indices = ious.max(dim=1)\n",
    "        \n",
    "        for pred_label, iou, max_index in zip(pred_labels, max_ious, max_indices):\n",
    "            if iou >= iou_threshold and pred_label == target_labels[max_index]:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "        \n",
    "        false_negatives += len(target_boxes) - (max_ious >= iou_threshold).sum()\n",
    "    \n",
    "    precision_result = true_positives / (true_positives + false_positives + 1e-8)\n",
    "    recall_result = true_positives / (true_positives + false_negatives + 1e-8)\n",
    "    F1_score = 2 * (precision_result * recall_result) / (precision_result + recall_result + 1e-8)\n",
    "    \n",
    "    return precision_result, recall_result, F1_score\n",
    "\n",
    "def box_iou(boxes1, boxes2):\n",
    "    area1 = box_area(boxes1)\n",
    "    area2 = box_area(boxes2)\n",
    "    \n",
    "    if boxes1.dim() == 1:\n",
    "        boxes1 = boxes1.unsqueeze(0)\n",
    "    if boxes2.dim() == 1:\n",
    "        boxes2 = boxes2.unsqueeze(0)\n",
    "    \n",
    "    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n",
    "    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n",
    "    \n",
    "    wh = (rb - lt).clamp(min=0)\n",
    "    inter = wh[:, :, 0] * wh[:, :, 1]\n",
    "    \n",
    "    union = area1[:, None] + area2 - inter\n",
    "    \n",
    "    iou = inter / (union + 1e-8)\n",
    "    return iou\n",
    "\n",
    "def box_area(boxes):\n",
    "    if boxes.dim() == 1:\n",
    "        return (boxes[2] - boxes[0]) * (boxes[3] - boxes[1])\n",
    "    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "204e7e4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T04:57:31.494912Z",
     "iopub.status.busy": "2024-08-21T04:57:31.494637Z",
     "iopub.status.idle": "2024-08-21T04:57:31.509234Z",
     "shell.execute_reply": "2024-08-21T04:57:31.508442Z"
    },
    "papermill": {
     "duration": 0.021467,
     "end_time": "2024-08-21T04:57:31.511142",
     "exception": false,
     "start_time": "2024-08-21T04:57:31.489675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import FrozenBatchNorm2d\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=0.25, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "class FastRCNNPredictorWithFocalLoss(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(FastRCNNPredictorWithFocalLoss, self).__init__()\n",
    "        self.cls_score = nn.Linear(in_channels, num_classes)\n",
    "        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n",
    "        self.focal_loss = FocalLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 4:\n",
    "            torch._assert(\n",
    "                x.shape[1] <= self.cls_score.weight.shape[1],\n",
    "                f\"The model has been trained with {self.cls_score.weight.shape[1]} inputs, \"\n",
    "                f\"but got {x.shape[1]} inputs\"\n",
    "            )\n",
    "            x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.flatten(1)\n",
    "        scores = self.cls_score(x)\n",
    "        labels = torch.zeros(scores.shape[0], dtype=torch.long, device=scores.device)\n",
    "        loss_cls = self.focal_loss(scores, labels)\n",
    "        bbox_pred = self.bbox_pred(x)\n",
    "        return scores, bbox_pred\n",
    "    \n",
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='FasterRCNN_ResNet50_FPN_Weights.DEFAULT')\n",
    "\n",
    "    # Adjust anchor sizes in RPN to better match the small size of the objects\n",
    "    backbone = model.backbone\n",
    "    feature_maps = backbone(torch.zeros(1, 3, 224, 224))  # Replace 224 with your image size\n",
    "    num_feature_maps = len(feature_maps)\n",
    "\n",
    "    # Adjust anchor sizes in RPN to better match the small size of the objects\n",
    "    anchor_sizes = tuple(((8, 16, 32),) for _ in range(num_feature_maps))\n",
    "    aspect_ratios = tuple(((0.5, 1.0, 2.0),) for _ in range(num_feature_maps))\n",
    "    anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=aspect_ratios)\n",
    "    model.rpn.anchor_generator = anchor_generator\n",
    "\n",
    "#     in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "#     additional_layers = nn.Sequential(\n",
    "#         nn.Linear(in_features, 1024),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Dropout(0.1),\n",
    "#     )\n",
    "\n",
    "#    # Create a new box predictor with additional layers and Focal Loss\n",
    "#     predictor = FastRCNNPredictorWithFocalLoss(in_features, num_classes)\n",
    "#     model.roi_heads.box_predictor = nn.Sequential(\n",
    "#         additional_layers,\n",
    "#         predictor\n",
    "#     )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dad52780",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-21T04:57:31.520351Z",
     "iopub.status.busy": "2024-08-21T04:57:31.519835Z",
     "iopub.status.idle": "2024-08-21T04:57:31.538610Z",
     "shell.execute_reply": "2024-08-21T04:57:31.537874Z"
    },
    "papermill": {
     "duration": 0.025534,
     "end_time": "2024-08-21T04:57:31.540483",
     "exception": false,
     "start_time": "2024-08-21T04:57:31.514949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            for output, target in zip(outputs, targets):\n",
    "                if target['boxes'].numel() > 0:\n",
    "                    pred_boxes = output['boxes'].cpu()\n",
    "                    pred_labels = output['labels'].cpu()\n",
    "                    pred_scores = output['scores'].cpu()\n",
    "\n",
    "                    target_boxes = target['boxes'].cpu()\n",
    "                    target_labels = target['labels'].cpu()\n",
    "\n",
    "                    all_predictions.append({\n",
    "                        'boxes': pred_boxes,\n",
    "                        'labels': pred_labels,\n",
    "                        'scores': pred_scores\n",
    "                    })\n",
    "                    all_targets.append({\n",
    "                        'boxes': target_boxes,\n",
    "                        'labels': target_labels\n",
    "                    })\n",
    "\n",
    "    # Calculate mAP\n",
    "    mAP_result = calculate_mAP(all_predictions, all_targets)['map_50'].item()\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision_result, recall_result, F1_score = calculate_metrics(all_predictions, all_targets)\n",
    "    \n",
    "    return precision_result, recall_result, mAP_result, F1_score\n",
    "\n",
    "# Hàm huấn luyện model\n",
    "def train_model(model, train_data_loader, val_data_loader, device, num_epochs):\n",
    "    model.to(device)\n",
    "    best_metrics = \"null\"\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "    \n",
    "    best_precision = 0.0  # Khởi tạo giá trị độ chính xác tốt nhất\n",
    "    best_model_state = None  # Biến để lưu trạng thái mô hình tốt nhất\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_pbar = tqdm(enumerate(train_data_loader), total=len(train_data_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for i, (images, targets) in train_pbar:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Filter out images with no objects\n",
    "            valid_images = []\n",
    "            valid_targets = []\n",
    "            for img, target in zip(images, targets):\n",
    "                if target['boxes'].numel() > 0:\n",
    "                    valid_images.append(img)\n",
    "                    valid_targets.append(target)\n",
    "            \n",
    "            if len(valid_images) == 0:\n",
    "                continue  # Skip this batch if all images have no objects\n",
    "            \n",
    "            loss_dict = model(valid_images, valid_targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_pbar.set_postfix({\"Loss\": losses.item()})  # Cập nhật thông tin loss trên thanh tiến trình\n",
    "\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Đánh giá mô hình trên tập validation\n",
    "        precision_result, recall_result, mAP_result, F1_score = evaluate_model(model, val_data_loader, device)\n",
    "        metrics = f\"precision: {precision_result:.4f}, recall: {recall_result:.4f}, mAP_50: {mAP_result:.4f}, F1: {F1_score:.4f}\"\n",
    "        print(metrics)\n",
    "        print(\"best metrics:\", best_metrics, \"\\n\")\n",
    "        \n",
    "        # Lưu mô hình tốt nhất\n",
    "        if precision_result > best_precision:\n",
    "            best_precision = precision_result\n",
    "            best_model_state = model.state_dict()  # Lưu trạng thái mô hình\n",
    "            best_metrics = metrics\n",
    "            torch.save(best_model_state, 'best_model_res_nes.pth')\n",
    "            print(\"Saved best model with precision:\", best_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c4b1e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-21T04:57:31.549160Z",
     "iopub.status.busy": "2024-08-21T04:57:31.548924Z",
     "iopub.status.idle": "2024-08-21T07:06:57.839187Z",
     "shell.execute_reply": "2024-08-21T07:06:57.838155Z"
    },
    "papermill": {
     "duration": 7766.667515,
     "end_time": "2024-08-21T07:06:58.211865",
     "exception": false,
     "start_time": "2024-08-21T04:57:31.544350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:01<00:00, 167MB/s]\n",
      "Epoch 1/20: 100%|██████████| 117/117 [06:23<00:00,  3.28s/it, Loss=0.437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.5257, recall: 0.7637, mAP_50: 0.4764, F1: 0.6228\n",
      "best metrics: null \n",
      "\n",
      "Saved best model with precision: 0.5257498585157733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 117/117 [05:41<00:00,  2.92s/it, Loss=0.348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.5704, recall: 0.7197, mAP_50: 0.4831, F1: 0.6364\n",
      "best metrics: precision: 0.5257, recall: 0.7637, mAP_50: 0.4764, F1: 0.6228 \n",
      "\n",
      "Saved best model with precision: 0.5704203743462092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 117/117 [05:38<00:00,  2.89s/it, Loss=0.234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6203, recall: 0.6726, mAP_50: 0.4849, F1: 0.6454\n",
      "best metrics: precision: 0.5704, recall: 0.7197, mAP_50: 0.4831, F1: 0.6364 \n",
      "\n",
      "Saved best model with precision: 0.6203333333312656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 117/117 [05:37<00:00,  2.89s/it, Loss=0.303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6017, recall: 0.6875, mAP_50: 0.4859, F1: 0.6417\n",
      "best metrics: precision: 0.6203, recall: 0.6726, mAP_50: 0.4849, F1: 0.6454 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 117/117 [05:38<00:00,  2.90s/it, Loss=0.285]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6227, recall: 0.6687, mAP_50: 0.4859, F1: 0.6449\n",
      "best metrics: precision: 0.6203, recall: 0.6726, mAP_50: 0.4849, F1: 0.6454 \n",
      "\n",
      "Saved best model with precision: 0.6226604278054055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 117/117 [05:36<00:00,  2.87s/it, Loss=0.207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6055, recall: 0.6821, mAP_50: 0.4861, F1: 0.6415\n",
      "best metrics: precision: 0.6227, recall: 0.6687, mAP_50: 0.4859, F1: 0.6449 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 117/117 [05:40<00:00,  2.91s/it, Loss=0.346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6153, recall: 0.6741, mAP_50: 0.4862, F1: 0.6434\n",
      "best metrics: precision: 0.6227, recall: 0.6687, mAP_50: 0.4859, F1: 0.6449 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 117/117 [05:41<00:00,  2.92s/it, Loss=0.301]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6094, recall: 0.6784, mAP_50: 0.4863, F1: 0.6421\n",
      "best metrics: precision: 0.6227, recall: 0.6687, mAP_50: 0.4859, F1: 0.6449 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 117/117 [05:42<00:00,  2.93s/it, Loss=0.525]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6138, recall: 0.6750, mAP_50: 0.4863, F1: 0.6430\n",
      "best metrics: precision: 0.6227, recall: 0.6687, mAP_50: 0.4859, F1: 0.6449 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 117/117 [05:40<00:00,  2.91s/it, Loss=0.402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6132, recall: 0.6755, mAP_50: 0.4862, F1: 0.6429\n",
      "best metrics: precision: 0.6227, recall: 0.6687, mAP_50: 0.4859, F1: 0.6449 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 117/117 [05:41<00:00,  2.92s/it, Loss=0.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6118, recall: 0.6765, mAP_50: 0.4863, F1: 0.6425\n",
      "best metrics: precision: 0.6227, recall: 0.6687, mAP_50: 0.4859, F1: 0.6449 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 117/117 [05:43<00:00,  2.93s/it, Loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6128, recall: 0.6757, mAP_50: 0.4862, F1: 0.6427\n",
      "best metrics: precision: 0.6227, recall: 0.6687, mAP_50: 0.4859, F1: 0.6449 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 117/117 [05:41<00:00,  2.92s/it, Loss=0.447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6128, recall: 0.6757, mAP_50: 0.4862, F1: 0.6427\n",
      "best metrics: precision: 0.6227, recall: 0.6687, mAP_50: 0.4859, F1: 0.6449 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 117/117 [05:40<00:00,  2.91s/it, Loss=0.386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6128, recall: 0.6755, mAP_50: 0.4862, F1: 0.6426\n",
      "best metrics: precision: 0.6227, recall: 0.6687, mAP_50: 0.4859, F1: 0.6449 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 117/117 [05:39<00:00,  2.90s/it, Loss=0.166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6128, recall: 0.6755, mAP_50: 0.4862, F1: 0.6426\n",
      "best metrics: precision: 0.6227, recall: 0.6687, mAP_50: 0.4859, F1: 0.6449 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 117/117 [05:39<00:00,  2.90s/it, Loss=0.334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6128, recall: 0.6755, mAP_50: 0.4862, F1: 0.6426\n",
      "best metrics: precision: 0.6227, recall: 0.6687, mAP_50: 0.4859, F1: 0.6449 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 117/117 [05:36<00:00,  2.87s/it, Loss=0.296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6128, recall: 0.6755, mAP_50: 0.4862, F1: 0.6426\n",
      "best metrics: precision: 0.6227, recall: 0.6687, mAP_50: 0.4859, F1: 0.6449 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 117/117 [05:40<00:00,  2.91s/it, Loss=0.197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6128, recall: 0.6755, mAP_50: 0.4862, F1: 0.6426\n",
      "best metrics: precision: 0.6227, recall: 0.6687, mAP_50: 0.4859, F1: 0.6449 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 117/117 [05:42<00:00,  2.93s/it, Loss=0.456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6128, recall: 0.6755, mAP_50: 0.4862, F1: 0.6426\n",
      "best metrics: precision: 0.6227, recall: 0.6687, mAP_50: 0.4859, F1: 0.6449 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 117/117 [05:41<00:00,  2.92s/it, Loss=0.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.6128, recall: 0.6755, mAP_50: 0.4862, F1: 0.6426\n",
      "best metrics: precision: 0.6227, recall: 0.6687, mAP_50: 0.4859, F1: 0.6449 \n",
      "\n",
      "\n",
      "\n",
      " Efficient Net Evaluate on test set: \n",
      "\n",
      "precision: 0.6113, recall: 0.6870, mAP_50: 0.4836, F1: 0.6469\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=6, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=6, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=6, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = get_model(num_classes)\n",
    "\n",
    "# Huấn luyện model trên tập train\n",
    "train_model(model, train_loader, val_loader, device, num_epochs=20)\n",
    "\n",
    "precision_result, recall_result, mAP_result, F1_score = evaluate_model(model, test_loader, device)\n",
    "\n",
    "print(\"\\n\\n Efficient Net Evaluate on test set: \\n\")\n",
    "metrics = f\"precision: {precision_result:.4f}, recall: {recall_result:.4f}, mAP_50: {mAP_result:.4f}, F1: {F1_score:.4f}\"\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5447766,
     "sourceId": 9205470,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 98058,
     "modelInstanceId": 73186,
     "sourceId": 87130,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7793.089222,
   "end_time": "2024-08-21T07:07:00.816962",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-21T04:57:07.727740",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
