{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9260531,"sourceType":"datasetVersion","datasetId":5589550}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pycocotools","metadata":{"execution":{"iopub.status.busy":"2024-08-28T06:20:40.780301Z","iopub.execute_input":"2024-08-28T06:20:40.780687Z","iopub.status.idle":"2024-08-28T06:20:54.741840Z","shell.execute_reply.started":"2024-08-28T06:20:40.780633Z","shell.execute_reply":"2024-08-28T06:20:54.740717Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pycocotools\n  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.7.5)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.26.4)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (9.5.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\nDownloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.8/427.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: pycocotools\nSuccessfully installed pycocotools-2.0.8\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport yaml\nimport torch\nimport torchvision\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchmetrics.detection.mean_ap import MeanAveragePrecision","metadata":{"execution":{"iopub.status.busy":"2024-08-28T06:20:54.744062Z","iopub.execute_input":"2024-08-28T06:20:54.744459Z","iopub.status.idle":"2024-08-28T06:21:01.656504Z","shell.execute_reply.started":"2024-08-28T06:20:54.744424Z","shell.execute_reply":"2024-08-28T06:21:01.655638Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import random\n\nclass ToTensor:\n    def __call__(self, image, target):\n        image = torchvision.transforms.functional.to_tensor(image)\n        return image, target\n\n# Lớp Dataset tùy chỉnh cho YOLO\nclass YOLODataset(Dataset):\n    def __init__(self, image_path, anotation_path, transforms=None, keep_rate=(10, 10)):\n        self.transforms = transforms\n        self.img_path = image_path\n        self.annotation_path = anotation_path\n        self.keep_rate = keep_rate\n\n    def __getitem__(self, idx):\n        img_path = self.img_path[idx]\n        annotation_path = self.annotation_path[idx]\n        \n        img = Image.open(img_path).convert(\"RGB\")\n        img = torchvision.transforms.Resize((1024, 1024))(img)\n        w, h = img.size\n\n        boxes = []\n        labels = []\n        \n        with open(annotation_path) as f:\n            for line in f:\n                parts = line.strip().split()\n                class_id = int(parts[0])\n                if random.random() > self.keep_rate[class_id]:\n                    continue\n                    \n                x_center, y_center, width, height = map(float, parts[1:])\n                \n                # Convert from YOLO format to (xmin, ymin, xmax, ymax)\n                xmin = (x_center - width / 2) * w\n                ymin = (y_center - height / 2) * h\n                xmax = (x_center + width / 2) * w\n                ymax = (y_center + height / 2) * h\n\n                boxes.append([xmin, ymin, xmax, ymax])\n                labels.append(class_id)\n\n        if len(boxes) == 0:\n            # If there are no boxes, return a dummy box to avoid errors\n            boxes = torch.tensor([[0, 0, 1, 1]], dtype=torch.float32)\n            labels = torch.tensor([0], dtype=torch.int64)  # Use a background class (0)\n        else:\n            boxes = torch.as_tensor(boxes, dtype=torch.float32)\n            labels = torch.as_tensor(labels, dtype=torch.int64)\n\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels\n        }\n\n        if self.transforms:\n            img, target = self.transforms(img, target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.img_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-28T06:21:01.657684Z","iopub.execute_input":"2024-08-28T06:21:01.658090Z","iopub.status.idle":"2024-08-28T06:21:01.671911Z","shell.execute_reply.started":"2024-08-28T06:21:01.658064Z","shell.execute_reply":"2024-08-28T06:21:01.670974Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def get_path(img_path):\n    images = []\n    for root, dirs, files in os.walk(img_path):\n        for file in files:\n            if file.endswith(\".JPG\"):\n                full_path = os.path.join(root, file)\n                images.append(full_path)\n    labels = [path.replace('images', 'labels').replace('.JPG', '.txt') for path in images]\n    return images, labels\n\ntrain_image_path, train_annotation_path = get_path(\"/kaggle/input/riceseed/images/train\")\nval_image_path, val_annotation_path = get_path(\"/kaggle/input/riceseed/images/val\")\ntest_image_path, test_annotation_path = get_path(\"/kaggle/input/riceseed/images/test\")\n\nval_image_path = val_image_path + test_image_path\nval_annotation_path = val_annotation_path + test_annotation_path\n\nnum_classes = 2\n\ntrain_dataset = YOLODataset(train_image_path, train_annotation_path, transforms=ToTensor(), keep_rate=(10, 10))\nval_dataset = YOLODataset(val_image_path, val_annotation_path, transforms=ToTensor())\n# test_dataset = YOLODataset(test_image_path, test_annotation_path, transforms=ToTensor())","metadata":{"execution":{"iopub.status.busy":"2024-08-28T06:21:01.674559Z","iopub.execute_input":"2024-08-28T06:21:01.674921Z","iopub.status.idle":"2024-08-28T06:21:01.903179Z","shell.execute_reply.started":"2024-08-28T06:21:01.674889Z","shell.execute_reply":"2024-08-28T06:21:01.902379Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def calculate_mAP(predictions, targets):\n    metric = MeanAveragePrecision()\n    metric.update(predictions, targets)\n    return metric.compute()\n\ndef calculate_metrics(predictions, targets, iou_threshold=0.5):\n    true_positives = 0\n    false_positives = 0\n    false_negatives = 0\n    \n    for pred, target in zip(predictions, targets):\n        pred_boxes = pred['boxes']\n        pred_labels = pred['labels']\n        target_boxes = target['boxes']\n        target_labels = target['labels']\n        \n        if len(pred_boxes) == 0 or len(target_boxes) == 0:\n            false_positives += len(pred_boxes)\n            false_negatives += len(target_boxes)\n            continue\n        \n        ious = box_iou(pred_boxes, target_boxes)\n        max_ious, max_indices = ious.max(dim=1)\n        \n        for pred_label, iou, max_index in zip(pred_labels, max_ious, max_indices):\n            if iou >= iou_threshold and pred_label == target_labels[max_index]:\n                true_positives += 1\n            else:\n                false_positives += 1\n        \n        false_negatives += len(target_boxes) - (max_ious >= iou_threshold).sum()\n    \n    precision_result = true_positives / (true_positives + false_positives + 1e-8)\n    recall_result = true_positives / (true_positives + false_negatives + 1e-8)\n    F1_score = 2 * (precision_result * recall_result) / (precision_result + recall_result + 1e-8)\n    \n    return precision_result, recall_result, F1_score\n\ndef box_iou(boxes1, boxes2):\n    area1 = box_area(boxes1)\n    area2 = box_area(boxes2)\n    \n    if boxes1.dim() == 1:\n        boxes1 = boxes1.unsqueeze(0)\n    if boxes2.dim() == 1:\n        boxes2 = boxes2.unsqueeze(0)\n    \n    lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])\n    rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])\n    \n    wh = (rb - lt).clamp(min=0)\n    inter = wh[:, :, 0] * wh[:, :, 1]\n    \n    union = area1[:, None] + area2 - inter\n    \n    iou = inter / (union + 1e-8)\n    return iou\n\ndef box_area(boxes):\n    if boxes.dim() == 1:\n        return (boxes[2] - boxes[0]) * (boxes[3] - boxes[1])\n    return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])","metadata":{"execution":{"iopub.status.busy":"2024-08-28T06:21:01.904244Z","iopub.execute_input":"2024-08-28T06:21:01.904538Z","iopub.status.idle":"2024-08-28T06:21:01.918943Z","shell.execute_reply.started":"2024-08-28T06:21:01.904511Z","shell.execute_reply":"2024-08-28T06:21:01.918058Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torchvision\nimport torch\nimport torch.nn as nn\nfrom torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn_v2, FastRCNNPredictor\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.ops import FrozenBatchNorm2d\nimport torch.nn.functional as F\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2, alpha=0.25, reduction='mean'):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        if self.reduction == 'mean':\n            return focal_loss.mean()\n        elif self.reduction == 'sum':\n            return focal_loss.sum()\n        else:\n            return focal_loss\n\nclass FastRCNNPredictorWithFocalLoss(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super(FastRCNNPredictorWithFocalLoss, self).__init__()\n        self.cls_score = nn.Linear(in_channels, num_classes)\n        self.bbox_pred = nn.Linear(in_channels, num_classes * 4)\n        self.focal_loss = FocalLoss()\n\n    def forward(self, x):\n        if x.dim() == 4:\n            torch._assert(\n                x.shape[1] <= self.cls_score.weight.shape[1],\n                f\"The model has been trained with {self.cls_score.weight.shape[1]} inputs, \"\n                f\"but got {x.shape[1]} inputs\"\n            )\n            x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n        x = x.flatten(1)\n        scores = self.cls_score(x)\n        labels = torch.zeros(scores.shape[0], dtype=torch.long, device=scores.device)\n        loss_cls = self.focal_loss(scores, labels)\n        bbox_pred = self.bbox_pred(x)\n        return scores, bbox_pred\n    \ndef get_model(num_classes):\n    anchor_generator = AnchorGenerator(\n        sizes=((32, 64),),  # Anchor widths: 32 and 64\n        aspect_ratios=((2.0, 1.0, 0.5),)  # Aspect ratios for (32x64, 64x64, 64x32)\n    )\n    \n    model = fasterrcnn_resnet50_fpn_v2(max_size=1024,\n                                        box_detections_per_img=100,\n                                        anchor_generator = anchor_generator,\n                                        weights='FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1',\n                                        weights_backbone='ResNet50_Weights.IMAGENET1K_V1', \n                                        trainable_backbone_layers = 5)  \n\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n\n#     additional_layers = nn.Sequential(\n#         nn.Linear(in_features, 1024),\n#         nn.ReLU(),\n#         nn.Dropout(0.3),\n#         nn.Linear(1024, 512),\n#         nn.ReLU(),\n#         nn.Dropout(0.3),\n#         nn.Linear(512, 256),\n#         nn.ReLU(),\n#         nn.Dropout(0.2),\n#         nn.Linear(256, 128),\n#         nn.ReLU(),\n#         nn.Dropout(0.1)\n#     )\n\n#     # Create a new box predictor with additional layers and Focal Loss\n    model.roi_heads.box_predictor = FastRCNNPredictorWithFocalLoss(in_features, num_classes)\n#     model.roi_heads.box_predictor = nn.Sequential(\n#         additional_layers,\n#         predictor\n#     )\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-08-28T06:21:01.920265Z","iopub.execute_input":"2024-08-28T06:21:01.920560Z","iopub.status.idle":"2024-08-28T06:21:01.936876Z","shell.execute_reply.started":"2024-08-28T06:21:01.920525Z","shell.execute_reply":"2024-08-28T06:21:01.935886Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport random\n\ndef evaluate_model(model, data_loader, device):\n    model.eval()\n    \n    all_predictions = []\n    all_targets = []\n    \n    with torch.no_grad():\n        for images, targets in data_loader:\n            images = list(img.to(device) for img in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            outputs = model(images)\n            \n            for output, target in zip(outputs, targets):\n                if target['boxes'].numel() > 0:\n                    pred_boxes = output['boxes'].cpu()\n                    pred_labels = output['labels'].cpu()\n                    pred_scores = output['scores'].cpu()\n\n                    target_boxes = target['boxes'].cpu()\n                    target_labels = target['labels'].cpu()\n\n                    all_predictions.append({\n                        'boxes': pred_boxes,\n                        'labels': pred_labels,\n                        'scores': pred_scores\n                    })\n                    all_targets.append({\n                        'boxes': target_boxes,\n                        'labels': target_labels\n                    })\n\n    # Calculate mAP\n    mAP_result = calculate_mAP(all_predictions, all_targets)['map_50'].item()\n    \n    # Calculate precision, recall, and F1 score\n    precision_result, recall_result, F1_score = calculate_metrics(all_predictions, all_targets)\n    \n    return precision_result, recall_result, mAP_result, F1_score\n\ndef train_model(model, train_data_loader, val_data_loader, device, num_epochs, accumulation_steps=8):\n    model.to(device)\n    best_metrics = \"null\"\n    params = [p for p in model.parameters() if p.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=0.003, momentum=0.9, weight_decay=0.0005)\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n    \n    best_precision = 0.0 \n    best_model_state = None  \n    loss = 100\n    for epoch in range(num_epochs):\n        model.train()\n        train_pbar = tqdm(enumerate(train_data_loader), total=len(train_data_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        \n        optimizer.zero_grad()  \n        \n        for i, (images, targets) in train_pbar:\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            # Filter out images with no objects\n            valid_images = []\n            valid_targets = []\n            for img, target in zip(images, targets):\n                if target['boxes'].numel() > 0:\n                    valid_images.append(img)\n                    valid_targets.append(target)\n            \n            if len(valid_images) == 0:\n                continue \n            \n            loss_dict = model(valid_images, valid_targets)\n            losses = sum(loss for loss in loss_dict.values())\n\n            losses.backward()  \n\n#             if (i + 1) % accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad() \n            loss = losses.item()\n\n            train_pbar.set_postfix({\"Loss\": losses.item()}) \n\n        lr_scheduler.step()\n        \n        precision_result, recall_result, mAP_result, F1_score = evaluate_model(model, val_data_loader, device)\n        metrics = f\"precision: {precision_result:.4f}, recall: {recall_result:.4f}, mAP_50: {mAP_result:.4f}, F1: {F1_score:.4f}\"\n        print(metrics)\n        \n        if precision_result > best_precision:\n            best_precision = precision_result\n            best_model_state = model.state_dict()  \n            best_metrics = metrics\n            torch.save(best_model_state, 'best_model_res_nes.pth')\n            print(\"Saved best model with precision:\", precision_result)\n        \n        print(\"best metrics:\", best_metrics, \"\\n\")\n  \n    if (i + 1) % accumulation_steps != 0:\n        optimizer.step()\n        optimizer.zero_grad() \n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-28T06:21:01.937974Z","iopub.execute_input":"2024-08-28T06:21:01.938248Z","iopub.status.idle":"2024-08-28T06:21:01.958898Z","shell.execute_reply.started":"2024-08-28T06:21:01.938225Z","shell.execute_reply":"2024-08-28T06:21:01.957899Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import gc\n\ngc.collect()\n\ntorch.cuda.empty_cache()\n\n\nbatch_size = 8\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel = get_model(num_classes)\n\ntrain_model(model, train_loader, val_loader, device, num_epochs=20)\n\nprecision_result, recall_result, mAP_result, F1_score = evaluate_model(model, test_loader, device)\n\nprint(\"\\n\\n Efficient Net Evaluate on test set: \\n\")\nmetrics = f\"precision: {precision_result:.4f}, recall: {recall_result:.4f}, mAP_50: {mAP_result:.4f}, F1: {F1_score:.4f}\"\nprint(metrics)","metadata":{"execution":{"iopub.status.busy":"2024-08-28T06:21:01.960075Z","iopub.execute_input":"2024-08-28T06:21:01.960400Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\n100%|██████████| 167M/167M [00:02<00:00, 77.5MB/s] \nEpoch 1/20: 100%|██████████| 88/88 [05:04<00:00,  3.46s/it, Loss=0.25] \n","output_type":"stream"},{"name":"stdout","text":"precision: 0.5786, recall: 0.7513, mAP_50: 0.4872, F1: 0.6537\nSaved best model with precision: 0.5785843324017432\nbest metrics: precision: 0.5786, recall: 0.7513, mAP_50: 0.4872, F1: 0.6537 \n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 88/88 [04:26<00:00,  3.03s/it, Loss=0.202]\n","output_type":"stream"},{"name":"stdout","text":"precision: 0.7250, recall: 0.6003, mAP_50: 0.4893, F1: 0.6568\nSaved best model with precision: 0.7250206440942919\nbest metrics: precision: 0.7250, recall: 0.6003, mAP_50: 0.4893, F1: 0.6568 \n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 88/88 [04:29<00:00,  3.06s/it, Loss=0.177]\n","output_type":"stream"},{"name":"stdout","text":"precision: 0.7752, recall: 0.5712, mAP_50: 0.4906, F1: 0.6577\nSaved best model with precision: 0.7752263192740666\nbest metrics: precision: 0.7752, recall: 0.5712, mAP_50: 0.4906, F1: 0.6577 \n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 88/88 [04:21<00:00,  2.97s/it, Loss=0.138]\n","output_type":"stream"},{"name":"stdout","text":"precision: 0.7321, recall: 0.5986, mAP_50: 0.4910, F1: 0.6587\nbest metrics: precision: 0.7752, recall: 0.5712, mAP_50: 0.4906, F1: 0.6577 \n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 88/88 [04:23<00:00,  3.00s/it, Loss=0.178]\n","output_type":"stream"},{"name":"stdout","text":"precision: 0.7455, recall: 0.5904, mAP_50: 0.4908, F1: 0.6590\nbest metrics: precision: 0.7752, recall: 0.5712, mAP_50: 0.4906, F1: 0.6577 \n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|██████████| 88/88 [04:28<00:00,  3.05s/it, Loss=0.157]\n","output_type":"stream"},{"name":"stdout","text":"precision: 0.7452, recall: 0.5897, mAP_50: 0.4909, F1: 0.6584\nbest metrics: precision: 0.7752, recall: 0.5712, mAP_50: 0.4906, F1: 0.6577 \n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 88/88 [04:25<00:00,  3.01s/it, Loss=0.126]\n","output_type":"stream"},{"name":"stdout","text":"precision: 0.7556, recall: 0.5835, mAP_50: 0.4909, F1: 0.6585\nbest metrics: precision: 0.7752, recall: 0.5712, mAP_50: 0.4906, F1: 0.6577 \n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 88/88 [04:26<00:00,  3.03s/it, Loss=0.0897]\n","output_type":"stream"},{"name":"stdout","text":"precision: 0.7359, recall: 0.5953, mAP_50: 0.4910, F1: 0.6582\nbest metrics: precision: 0.7752, recall: 0.5712, mAP_50: 0.4906, F1: 0.6577 \n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 100%|██████████| 88/88 [04:27<00:00,  3.04s/it, Loss=0.227]\n","output_type":"stream"},{"name":"stdout","text":"precision: 0.7620, recall: 0.5795, mAP_50: 0.4909, F1: 0.6584\nbest metrics: precision: 0.7752, recall: 0.5712, mAP_50: 0.4906, F1: 0.6577 \n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/20: 100%|██████████| 88/88 [04:25<00:00,  3.02s/it, Loss=0.142]\n","output_type":"stream"}]}]}